{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc8ce3a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所有库已成功导入！\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "from typing import List, Dict, Tuple\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"所有库已成功导入！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecb46635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置参数\n",
    "MODEL_NAME = \"deepseek-r1:5b\"\n",
    "OLLAMA_API_URL = \"http://localhost:11434/api/chat\"  # Ollama API默认地址\n",
    "TIMEOUT = 60\n",
    "RETRIES = 3\n",
    "NEWS_FILE = \"news_data.txt\"  # 新闻数据文件\n",
    "LABELS_FILE = \"labels.txt\"  # 标签文件\n",
    "OUTPUT_FILE = \"news_analysis_results.json\"\n",
    "LOG_FILE = \"model_inference.log\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "168b984e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log(message: str, level: str = \"INFO\"):\n",
    "    \"\"\"带时间戳的日志输出，同时写入日志文件\"\"\"\n",
    "    timestamp = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    log_msg = f\"[{timestamp}] [{level}] {message}\"\n",
    "    print(log_msg)\n",
    "\n",
    "    # 写入日志文件\n",
    "    with open(LOG_FILE, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(log_msg + \"\\n\")\n",
    "\n",
    "\n",
    "def call_ollama_api(prompt: str) -> str:\n",
    "    \"\"\"使用HTTP API调用Ollama模型\"\"\"\n",
    "    start_time = time.time()\n",
    "    log(f\"开始调用模型: {MODEL_NAME}\", \"DEBUG\")\n",
    "\n",
    "    for attempt in range(RETRIES):\n",
    "        try:\n",
    "            # 构建API请求\n",
    "            payload = {\n",
    "                \"model\": MODEL_NAME,\n",
    "                \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "                \"stream\": False  # 非流式响应\n",
    "            }\n",
    "\n",
    "            log(f\"发送API请求 (尝试 {attempt + 1}/{RETRIES})\", \"DEBUG\")\n",
    "            response = requests.post(\n",
    "                OLLAMA_API_URL,\n",
    "                json=payload,\n",
    "                timeout=TIMEOUT\n",
    "            )\n",
    "\n",
    "            duration = time.time() - start_time\n",
    "\n",
    "            if response.status_code != 200:\n",
    "                log(f\"API调用失败 (状态码 {response.status_code}): {response.text}\", \"ERROR\")\n",
    "                time.sleep(2)\n",
    "                continue\n",
    "\n",
    "            # 解析响应\n",
    "            data = response.json()\n",
    "            message_content = data.get(\"message\", {}).get(\"content\", \"\")\n",
    "\n",
    "            log(f\"模型返回成功 (长度: {len(message_content)} 字符, 耗时: {duration:.2f}秒)\", \"INFO\")\n",
    "            return message_content\n",
    "\n",
    "        except requests.Timeout:\n",
    "            log(f\"API请求超时 ({TIMEOUT}秒)\", \"ERROR\")\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            log(f\"API调用异常: {type(e).__name__} - {str(e)}\", \"ERROR\")\n",
    "            time.sleep(2)\n",
    "\n",
    "    log(\"达到最大重试次数，返回空结果\", \"ERROR\")\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def read_news_file() -> List[str]:\n",
    "    \"\"\"从文件读取新闻数据\"\"\"\n",
    "    log(f\"从文件读取新闻数据: {NEWS_FILE}\", \"INFO\")\n",
    "\n",
    "    try:\n",
    "        # 读取新闻数据\n",
    "        with open(NEWS_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "            news_list = [line.strip() for line in f.readlines() if line.strip()]\n",
    "\n",
    "        if not news_list:\n",
    "            log(\"新闻文件为空，程序退出\", \"ERROR\")\n",
    "            exit(1)\n",
    "\n",
    "        log(f\"成功读取 {len(news_list)} 条新闻\", \"INFO\")\n",
    "        return news_list\n",
    "\n",
    "    except Exception as e:\n",
    "        log(f\"读取新闻文件时出错: {str(e)}\", \"ERROR\")\n",
    "        exit(1)\n",
    "\n",
    "\n",
    "def read_labels_file() -> List[int]:\n",
    "    \"\"\"从文件读取标签数据\"\"\"\n",
    "    log(f\"从文件读取标签数据: {LABELS_FILE}\", \"INFO\")\n",
    "    try:\n",
    "        with open(LABELS_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "            labels = [int(line.strip()) for line in f.readlines() if line.strip().isdigit()]\n",
    "        if not labels:\n",
    "            log(\"标签文件为空或无有效标签，程序退出\", \"ERROR\")\n",
    "            exit(1)\n",
    "        if len(labels) != len(read_news_file()):\n",
    "            log(\"新闻数量与标签数量不匹配，程序退出\", \"ERROR\")\n",
    "            exit(1)\n",
    "        log(f\"成功读取 {len(labels)} 个标签\", \"INFO\")\n",
    "        return labels\n",
    "    except Exception as e:\n",
    "        log(f\"读取标签文件时出错: {str(e)}\", \"ERROR\")\n",
    "        exit(1)\n",
    "\n",
    "\n",
    "def analyze_news(news_list: List[str], task_type: str) -> List:\n",
    "    \"\"\"分析新闻的通用函数，支持不同任务类型\"\"\"\n",
    "    results = []\n",
    "    log(f\"开始执行任务: {task_type}\")\n",
    "\n",
    "    for i, news in enumerate(tqdm(news_list, desc=task_type)):\n",
    "        # 根据任务类型构建不同的prompt\n",
    "        if task_type == \"真假判别\":\n",
    "            prompt = f\"\"\"\n",
    "            请判断以下新闻的真假，仅输出0或1：\n",
    "            新闻：{news}\n",
    "            \"\"\"\n",
    "        elif task_type == \"情感分析\":\n",
    "            prompt = f\"\"\"\n",
    "            请分析以下新闻的情感倾向，仅输出\"积极\"、\"消极\"或\"中性\"：\n",
    "            新闻：{news}\n",
    "            \"\"\"\n",
    "        else:\n",
    "            log(f\"不支持的任务类型: {task_type}\", \"ERROR\")\n",
    "            continue\n",
    "\n",
    "        # 添加任务标识符到日志\n",
    "        log(f\"处理新闻 {i + 1}/{len(news_list)} (任务: {task_type})\", \"INFO\")\n",
    "\n",
    "        response = call_ollama_api(prompt)\n",
    "\n",
    "        if task_type == \"真假判别\":\n",
    "            result = parse_prediction(response)\n",
    "        elif task_type == \"情感分析\":\n",
    "            result = parse_sentiment(response)\n",
    "\n",
    "        results.append(result)\n",
    "\n",
    "        # 每处理5条新闻，显示一次进度\n",
    "        if (i + 1) % 5 == 0:\n",
    "            log(f\"已处理 {i + 1}/{len(news_list)} 条新闻\", \"INFO\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def parse_prediction(response: str) -> int:\n",
    "    \"\"\"解析模型返回的真假判别结果\"\"\"\n",
    "    match = re.search(r'\\b(0|1)\\b', response)\n",
    "    if match:\n",
    "        log(f\"解析预测结果: {match.group(1)}\", \"DEBUG\")\n",
    "        return int(match.group(1))\n",
    "    else:\n",
    "        log(f\"无法解析预测结果: {response[:50]}...\", \"WARNING\")\n",
    "        return 0\n",
    "\n",
    "\n",
    "def parse_sentiment(response: str) -> str:\n",
    "    \"\"\"解析模型返回的情感分析结果\"\"\"\n",
    "    if re.search(r'\\b(积极|正面|好|支持)\\b', response, re.IGNORECASE):\n",
    "        return \"积极\"\n",
    "    elif re.search(r'\\b(消极|负面|坏|反对)\\b', response, re.IGNORECASE):\n",
    "        return \"消极\"\n",
    "    else:\n",
    "        return \"中性\"\n",
    "\n",
    "\n",
    "def calculate_accuracy(predictions: List[int], labels: List[int]) -> Dict[str, float]:\n",
    "    \"\"\"计算准确率指标\"\"\"\n",
    "    total = len(labels)\n",
    "    if total == 0:\n",
    "        return {\"Accuracy\": 0, \"Accuracy_fake\": 0, \"Accuracy_true\": 0}\n",
    "\n",
    "    correct = sum(1 for p, t in zip(predictions, labels) if p == t)\n",
    "    total_fake = sum(1 for t in labels if t == 0)\n",
    "    total_true = sum(1 for t in labels if t == 1)\n",
    "\n",
    "    correct_fake = sum(1 for p, t in zip(predictions, labels) if p == 0 and t == 0)\n",
    "    correct_true = sum(1 for p, t in zip(predictions, labels) if p == 1 and t == 1)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    accuracy_fake = correct_fake / total_fake if total_fake > 0 else 0\n",
    "    accuracy_true = correct_true / total_true if total_true > 0 else 0\n",
    "\n",
    "    return {\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Accuracy_fake\": accuracy_fake,\n",
    "        \"Accuracy_true\": accuracy_true\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5c5bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"\\n=== 离线DeepSeek新闻分析系统 ===\")\n",
    "    print(f\"使用模型: {MODEL_NAME}\")\n",
    "    print(f\"API地址: {OLLAMA_API_URL}\")\n",
    "    print(f\"新闻文件: {NEWS_FILE}\")\n",
    "    print(f\"标签文件: {LABELS_FILE}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # 检查Ollama API是否可用\n",
    "    try:\n",
    "        response = requests.get(f\"{OLLAMA_API_URL}/../tags\", timeout=10)\n",
    "        if response.status_code != 200:\n",
    "            log(f\"无法连接到Ollama API: {response.text}\", \"ERROR\")\n",
    "            exit(1)\n",
    "\n",
    "        models = [model[\"name\"] for model in response.json().get(\"models\", [])]\n",
    "        if MODEL_NAME not in models:\n",
    "            log(f\"未找到模型 {MODEL_NAME}，请确保已正确下载\", \"ERROR\")\n",
    "            exit(1)\n",
    "\n",
    "        log(f\"成功连接到Ollama API，可用模型: {', '.join(models)}\", \"INFO\")\n",
    "    except Exception as e:\n",
    "        log(f\"检查Ollama API时出错: {str(e)}\", \"ERROR\")\n",
    "        exit(1)\n",
    "\n",
    "    # 从文件读取新闻\n",
    "    news_list = read_news_file()\n",
    "    # 从文件读取标签\n",
    "    labels = read_labels_file()\n",
    "\n",
    "    # 任务1：基础真假判别\n",
    "    print(\"\\n=== 执行任务1：基础真假判别 ===\")\n",
    "    predictions = analyze_news(news_list, \"真假判别\")\n",
    "\n",
    "    # 计算准确率\n",
    "    accuracy_metrics = calculate_accuracy(predictions, labels)\n",
    "    print(\"\\n=== 准确率计算结果 ===\")\n",
    "    print(f\"Accuracy: {accuracy_metrics['Accuracy']}\")\n",
    "    print(f\"Accuracy_fake: {accuracy_metrics['Accuracy_fake']}\")\n",
    "    print(f\"Accuracy_true: {accuracy_metrics['Accuracy_true']}\")\n",
    "\n",
    "    # 任务2：情感分析\n",
    "    print(\"\\n=== 执行任务2：情感分析 ===\")\n",
    "    sentiments = analyze_news(news_list, \"情感分析\")\n",
    "\n",
    "    # 保存结果\n",
    "    results = {\n",
    "        \"news_data\": news_list,\n",
    "        \"predictions\": predictions,\n",
    "        \"sentiments\": sentiments,\n",
    "        \"accuracy_metrics\": accuracy_metrics\n",
    "    }\n",
    "\n",
    "    with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"\\n结果已保存到: {OUTPUT_FILE}\")\n",
    "    print(f\"详细日志已保存到: {LOG_FILE}\")\n",
    "    print(\"分析完成！\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f06bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 准确率计算结果 ===\n",
      "accuracy: 0.51 accuracy_true: 0.55 accuracy_fake: 0.47\n"
     ]
    }
   ],
   "source": [
    "    print(\"\\n=== 准确率计算结果 ===\")\n",
    "    print(f\"Accuracy: {accuracy_metrics['Accuracy']}\")\n",
    "    print(f\"Accuracy_fake: {accuracy_metrics['Accuracy_fake']}\")\n",
    "    print(f\"Accuracy_true: {accuracy_metrics['Accuracy_true']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9bd5edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.63 accuracy_true: 0.58 accuracy_fake: 0.68\n"
     ]
    }
   ],
   "source": [
    "print(\"accuracy:\" ,126/200,\n",
    "        \"accuracy_true:\" ,58/100,\n",
    "        \"accuracy_fake:\" ,68/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f8844606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在加载新闻数据: news_data.txt\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'news_data.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 217\u001b[0m\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m- 假新闻准确率: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy_fake\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2%\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[21], line 175\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    173\u001b[0m news_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnews_data.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# 格式: 标签(0/1)\\t新闻内容\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m正在加载新闻数据: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnews_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 175\u001b[0m texts, true_labels \u001b[38;5;241m=\u001b[39m \u001b[43mload_news_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnews_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;66;03m# 确保数据加载成功\u001b[39;00m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m texts:\n",
      "Cell \u001b[1;32mIn[21], line 125\u001b[0m, in \u001b[0;36mload_news_data\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m    122\u001b[0m texts \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    123\u001b[0m labels \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 125\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f:\n\u001b[0;32m    127\u001b[0m         parts \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32md:\\AN\\envs\\cts\\lib\\site-packages\\IPython\\core\\interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m     )\n\u001b[1;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'news_data.txt'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class DeepSeekClient:\n",
    "    \"\"\"与本地部署的DeepSeek模型API交互的客户端\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        base_url: str = \"http://localhost:8000\",\n",
    "        api_key: Optional[str] = None,\n",
    "        timeout: int = 60,\n",
    "        max_retries: int = 3,\n",
    "        retry_delay: float = 1.0,\n",
    "    ):\n",
    "        \"\"\"初始化DeepSeek API客户端\"\"\"\n",
    "        self.base_url = base_url\n",
    "        self.api_key = api_key\n",
    "        self.timeout = timeout\n",
    "        self.max_retries = max_retries\n",
    "        self.retry_delay = retry_delay\n",
    "        self.headers = {\"Content-Type\": \"application/json\"}\n",
    "        \n",
    "        if api_key:\n",
    "            self.headers[\"Authorization\"] = f\"Bearer {api_key}\"\n",
    "    \n",
    "    def _make_request(self, endpoint: str, payload: Dict) -> Dict:\n",
    "        \"\"\"发送HTTP请求到DeepSeek API并处理重试\"\"\"\n",
    "        url = f\"{self.base_url}{endpoint}\"\n",
    "        retries = 0\n",
    "        \n",
    "        while retries <= self.max_retries:\n",
    "            try:\n",
    "                response = requests.post(\n",
    "                    url, headers=self.headers, json=payload, timeout=self.timeout\n",
    "                )\n",
    "                response.raise_for_status()\n",
    "                return response.json()\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                retries += 1\n",
    "                if retries > self.max_retries:\n",
    "                    raise\n",
    "                time.sleep(self.retry_delay * retries)\n",
    "        \n",
    "        raise Exception(\"请求DeepSeek API失败\")\n",
    "    \n",
    "    def chat_completion(self, messages: List[Dict], model: str = \"deepseek-chat\") -> str:\n",
    "        \"\"\"调用聊天完成API并返回文本内容\"\"\"\n",
    "        payload = {\n",
    "            \"model\": model,\n",
    "            \"messages\": messages,\n",
    "            \"temperature\": 0.2,  # 低温度以获得更确定性的回答\n",
    "            \"max_tokens\": 512\n",
    "        }\n",
    "        \n",
    "        response = self._make_request(\"/v1/chat/completions\", payload)\n",
    "        return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "class NewsAnalyzer:\n",
    "    \"\"\"新闻分析器，用于新闻真实性判别和情感分析\"\"\"\n",
    "    \n",
    "    def __init__(self, client: DeepSeekClient):\n",
    "        self.client = client\n",
    "    \n",
    "    def detect_fake_news(self, news_text: str) -> int:\n",
    "        \"\"\"仅基于内容判别新闻真假\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "        你是一个专业的新闻验证专家。请判断以下新闻是真新闻还是假新闻，并回答\"真新闻\"或\"假新闻\"。\n",
    "        新闻内容: \"{news_text}\"\n",
    "        回答:\n",
    "        \"\"\"\n",
    "        \n",
    "        response = self.client.chat_completion([{\"role\": \"user\", \"content\": prompt}])\n",
    "        return 1 if \"真新闻\" in response else 0\n",
    "    \n",
    "    def analyze_sentiment(self, news_text: str) -> str:\n",
    "        \"\"\"分析新闻的情感倾向\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "        请分析以下新闻的情感倾向，回答\"积极\"、\"消极\"或\"中性\"。\n",
    "        新闻内容: \"{news_text}\"\n",
    "        回答:\n",
    "        \"\"\"\n",
    "        \n",
    "        response = self.client.chat_completion([{\"role\": \"user\", \"content\": prompt}])\n",
    "        \n",
    "        if \"积极\" in response:\n",
    "            return \"positive\"\n",
    "        elif \"消极\" in response:\n",
    "            return \"negative\"\n",
    "        else:\n",
    "            return \"neutral\"\n",
    "    \n",
    "    def detect_fake_news_with_sentiment(self, news_text: str) -> int:\n",
    "        \"\"\"结合情感分析判别新闻真假\"\"\"\n",
    "        sentiment = self.analyze_sentiment(news_text)\n",
    "        \n",
    "        # 根据情感调整提示\n",
    "        sentiment_prompt = {\n",
    "            \"positive\": \"这条新闻具有积极的情感倾向\",\n",
    "            \"negative\": \"这条新闻具有消极的情感倾向\",\n",
    "            \"neutral\": \"这条新闻具有中性的情感倾向\"\n",
    "        }[sentiment]\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "        你是一个专业的新闻验证专家。请考虑以下新闻的情感倾向，并判断它是真新闻还是假新闻。\n",
    "        情感倾向: {sentiment_prompt}\n",
    "        新闻内容: \"{news_text}\"\n",
    "        回答\"真新闻\"或\"假新闻\":\n",
    "        \"\"\"\n",
    "        \n",
    "        response = self.client.chat_completion([{\"role\": \"user\", \"content\": prompt}])\n",
    "        return 1 if \"真新闻\" in response else 0\n",
    "\n",
    "def load_news_data(file_path: str) -> Tuple[List[str], List[int]]:\n",
    "    \"\"\"加载新闻数据和真实标签\"\"\"\n",
    "    texts = []\n",
    "    labels = []\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) >= 2:\n",
    "                label = int(parts[0])\n",
    "                text = parts[1]\n",
    "                texts.append(text)\n",
    "                labels.append(label)\n",
    "    \n",
    "    return texts, labels\n",
    "\n",
    "def calculate_metrics(y_true: List[int], y_pred: List[int]) -> Dict[str, float]:\n",
    "    \"\"\"计算准确率指标\"\"\"\n",
    "    # 总准确率\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    # 分别计算真假新闻的准确率\n",
    "    true_indices = [i for i, label in enumerate(y_true) if label == 1]\n",
    "    fake_indices = [i for i, label in enumerate(y_true) if label == 0]\n",
    "    \n",
    "    if true_indices:\n",
    "        accuracy_true = accuracy_score(\n",
    "            [y_true[i] for i in true_indices],\n",
    "            [y_pred[i] for i in true_indices]\n",
    "        )\n",
    "    else:\n",
    "        accuracy_true = 0.0\n",
    "    \n",
    "    if fake_indices:\n",
    "        accuracy_fake = accuracy_score(\n",
    "            [y_true[i] for i in fake_indices],\n",
    "            [y_pred[i] for i in fake_indices]\n",
    "        )\n",
    "    else:\n",
    "        accuracy_fake = 0.0\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"accuracy_true\": accuracy_true,\n",
    "        \"accuracy_fake\": accuracy_fake\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    # 初始化客户端和分析器\n",
    "    client = DeepSeekClient(base_url=\"http://localhost:8000\")  # 修改为你的API地址\n",
    "    analyzer = NewsAnalyzer(client)\n",
    "    \n",
    "    # 加载数据\n",
    "    news_file = \"news_data.txt\"  # 格式: 标签(0/1)\\t新闻内容\n",
    "    print(f\"正在加载新闻数据: {news_file}\")\n",
    "    texts, true_labels = load_news_data(news_file)\n",
    "    \n",
    "    # 确保数据加载成功\n",
    "    if not texts:\n",
    "        print(\"未找到新闻数据，请检查文件路径和格式!\")\n",
    "        return\n",
    "    \n",
    "    print(f\"已加载 {len(texts)} 条新闻，开始分析...\")\n",
    "    \n",
    "    # 方法1: 仅基于内容判别\n",
    "    print(\"\\n=== 方法1: 仅基于内容判别新闻真假 ===\")\n",
    "    predictions_method1 = []\n",
    "    \n",
    "    for text in tqdm(texts, desc=\"分析中\"):\n",
    "        pred = analyzer.detect_fake_news(text)\n",
    "        predictions_method1.append(pred)\n",
    "        time.sleep(0.5)  # 避免请求过于频繁\n",
    "    \n",
    "    metrics1 = calculate_metrics(true_labels, predictions_method1)\n",
    "    print_metrics(metrics1, \"方法1\")\n",
    "    \n",
    "    # 方法2: 结合情感分析判别\n",
    "    print(\"\\n=== 方法2: 结合情感分析判别新闻真假 ===\")\n",
    "    predictions_method2 = []\n",
    "    \n",
    "    for text in tqdm(texts, desc=\"分析中\"):\n",
    "        pred = analyzer.detect_fake_news_with_sentiment(text)\n",
    "        predictions_method2.append(pred)\n",
    "        time.sleep(0.5)  # 避免请求过于频繁\n",
    "    \n",
    "    metrics2 = calculate_metrics(true_labels, predictions_method2)\n",
    "    print_metrics(metrics2, \"方法2\")\n",
    "    \n",
    "   \n",
    "def print_metrics(metrics: Dict[str, float], method_name: str):\n",
    "    \"\"\"打印准确率指标\"\"\"\n",
    "    print(f\"{method_name} 准确率:\")\n",
    "    print(f\"- 总准确率: {metrics['accuracy']:.2%}\")\n",
    "    print(f\"- 真新闻准确率: {metrics['accuracy_true']:.2%}\")\n",
    "    print(f\"- 假新闻准确率: {metrics['accuracy_fake']:.2%}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981a4643",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
